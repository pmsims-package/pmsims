---
title: "Getting started with pmsims"
---

```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6, fig.height = 4,
  warning = FALSE,
  message = FALSE
)
```

## What pmsims does

**pmsims** estimates the **minimum sample size** needed to develop a
prediction model to achieve a target level of performance **with
assurance**. Rather than relying on simple rules of thumb or closed‑form
formulae, pmsims uses **simulation** to:

-   Generate synthetic datasets that reflect your target setting
    (outcome type, prevalence or $R^2$, signal vs noise predictors);
-   Fit a specified **model** (e.g., logistic regression or linear
    regression);
-   Evaluate a chosen **performance metric** (e.g., calibration slope,
    AUC); and
-   Trace a **learning curve** of performance as the training size
    increases.

```{mermaid}
#| eval: false
flowchart LR
    A[User inputs] --> B[Data generator]
    A --> C[Prediction model]
    A --> D[Metrics]

    B --> E[Engine]
    C --> E
    D --> E

    E --> F[N*]

    style A fill:#f4cbd4,stroke:#7f2742,stroke-width:2px,color:#3b0a23
    style B fill:#cbd9f4,stroke:#597fb0,stroke-width:2px
    style C fill:#b8d3d3,stroke:#447273,stroke-width:2px
    style D fill:#f9e6a3,stroke:#b89b30,stroke-width:2px
    style E fill:#ffffff,stroke:#555,stroke-width:2px
    style F fill:#ffffff,stroke:#000,stroke-width:2px
```

The recommended design objective is **assurance**: the **smallest** $n$
such that a high proportion of repeated studies (e.g., 80%) meet the
target performance. In pmsims, this is implemented via the **20th
percentile** of the simulated performance distribution at each $n$.

## Required inputs at a glance

There are three wrapper functions for binary, continuous, and survival
outcomes, respectively:

- `simulate_binary()`
- `simulate_continuous()`
- `simulate_survival()`

All three functions share the same basic structure. The table below
lists the key inputs.

| Argument | <div style="width:200px">Applies to</div> | <div style="width:90px">Req</div> | Default | Description |
|-----------|-------------|:---:|----------|--------------|
| `signal_parameters` | all | ✓ | — | *(int)* Number of **true signal** predictors associated with the outcome. |
| `noise_parameters` | all |  | `0` | *(int)* Number of **noise** predictors unrelated to the outcome. |
| `predictor_type` | all |  | `"continuous"` | *(chr)* Type of simulated predictors (`"continuous"` or `"binary"`). |
| `binary_predictor_prevalence` | all |  | `NULL` | *(num 0–1)* Prevalence for binary predictors (used only if `predictor_type = "binary"`). |
| `outcome_prevalence` | binary only | ✓ | — | *(num 0–1)* Target prevalence of the binary outcome. |
| `large_sample_cstatistic` | binary only | ✓ | — | *(num 0–1)* Expected **C-statistic** for a very large training sample. |
| `large_sample_rsquared` | continuous only | ✓ | — | *(num 0–1)* Expected $R^2$ for a very large training sample. |
| `large_sample_cindex` | survival only | ✓ | — | *(num 0–1)* Expected **concordance index** for a very large training sample. |
| `model` | all |  | `"glm"` / `"lm"` / `"coxph"` | *(chr)* Model used for fitting (e.g. logistic, linear, or Cox). |
| `metric` | all |  | `"calibration_slope"` | *(chr)* **Performance metric** used to estimate the **minimum required sample size** (e.g. calibration slope, $R^2$, C-statistic). |
| `minimum_acceptable_performance` | all | ✓ | — | *(num)* **Minimum acceptable performance** in the **units of the chosen metric** (e.g. calibration slope ≥ 0.9). |
| `n_reps_total` | all | ✓ | `1000` | *(int)* Total number of simulation replications. |
| `mean_or_assurance` | all |  | `"assurance"` | *(chr)* Criterion for summarising results; `"assurance"` recommended. |

> Notes:
>
> - The engine automatically tunes the data generator so the
>   chosen model reaches the specified **large‑sample performance**
>   (C‑statistic or $R^2$) on very large samples.
> - For reproducibility, set a random seed (`set.seed()`).

## Installation

```{r}
#| eval: false
# install.packages("remotes")
# remotes::install_github("pmsims-package/pmsims")
library(pmsims)
```

```{r}
#| echo: false
devtools::load_all()
```

## Binary-outcome example

We target the smallest *n* that meets the **assurance** criterion.

```{r}
set.seed(123)

binary_example <- simulate_binary(
  signal_parameters = 15,
  noise_parameters  = 0,
  predictor_type = "continuous",
  binary_predictor_prevalence = NULL,
  outcome_prevalence = 0.20,
  large_sample_cstatistic = 0.80,
  model = "glm",
  metric = "calibration_slope",
  minimum_acceptable_performance = 0.90,
  n_reps_total = 1000,
  mean_or_assurance = "assurance"
)

binary_example
```

Plot the estimated learning curve and identified sample size:

```{r}
plot(binary_example)
```

## Continuous-outcome example

```{r}
#| eval: false
continuous_example <- simulate_continuous(
  signal_parameters = 15,
  noise_parameters = 0,
  predictor_type = "continuous",
  large_sample_rsquared = 0.50,
  model = "lm",
  metric = "calibration_slope",
  minimum_acceptable_performance = 0.90,
  n_reps_total = 1000,
  mean_or_assurance = "assurance"
)

continuous_example
```

```{r}
#| eval: false
plot(continuous_example)
```

## Session info

```{r}
sessionInfo()
```
